# Backend Dockerfile - PRODUCTION OPTIMIZED with HTTP/2 & Connection Pooling
FROM python:3.11-slim

WORKDIR /app

# Install minimal system dependencies + HTTP/2 support
RUN apt-get update && apt-get install -y --no-install-recommends \
    gcc \
    libgomp1 \
    curl \
    libnghttp2-dev \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

# Copy requirements and install Python dependencies
COPY backend/requirements.docker.txt requirements.txt

# Install PyTorch CPU-only FIRST to avoid 900MB GPU version
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir --timeout 300 --retries 10 torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu

# Install other dependencies with increased timeout and retries
RUN pip install --no-cache-dir --timeout 300 --retries 10 -r requirements.txt

# Set environment variables for MAXIMUM PERFORMANCE
# ðŸš€ PRODUCTION OPTIMIZED: HTTP/2 + Connection Pooling (2000 max connections)
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONPATH=/app \
    # CPU Optimization - INCREASED for parallel processing
    OMP_NUM_THREADS=16 \
    MKL_NUM_THREADS=16 \
    OPENBLAS_NUM_THREADS=16 \
    NUMEXPR_NUM_THREADS=16 \
    TORCH_NUM_THREADS=16 \
    # Disable warnings for cleaner logs
    TF_CPP_MIN_LOG_LEVEL=2 \
    # HTTP/2 & Connection Pooling Configuration
    HTTPX_HTTP2=1 \
    HTTPX_MAX_CONNECTIONS=2000 \
    HTTPX_MAX_KEEPALIVE=500 \
    HTTPX_KEEPALIVE_EXPIRY=180 \
    # Batch configuration for optimal performance
    BULK_UPLOAD_BATCH_SIZE=60 \
    EMBEDDING_BATCH_SIZE=15 \
    # Adaptive embedding batch sizes
    EMBEDDING_BATCH_SMALL=128 \
    EMBEDDING_BATCH_MEDIUM=64 \
    EMBEDDING_BATCH_LARGE=32

# Create necessary directories
RUN mkdir -p /app/chroma_db /app/cache

# Copy backend code
COPY backend/ .

# Pre-download embedding model to speed up first run
RUN python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('all-MiniLM-L6-v2')"

# Health check with longer intervals to avoid interference
HEALTHCHECK --interval=60s --timeout=10s --start-period=120s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Expose port
EXPOSE 8000

# PRODUCTION MODE: Multiple workers with optimal configuration
# ðŸš€ 8 workers = optimal for 60 parallel fetch + 15 parallel embedding
# --timeout-keep-alive 120 for long-running requests (large file processing)
# --limit-concurrency 1000 for high throughput
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "8", "--timeout-keep-alive", "120", "--limit-concurrency", "1000"]